[
{"paper":"Attention Is All You Need","question":"What makes the Transformer different from prior seq2seq models?","answer":"It models sequences using only self-attention and feed-forward layers, dispensing with any recurrence or convolution.","gold_chunks":["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions.","Self-attention allows modeling dependencies between all positions in the input sequence in a single layer."]},
{"paper":"Attention Is All You Need","question":"Define scaled dot-product attention.","answer":"It is softmax(QKᵀ/√dₖ)V, where scaling by √dₖ stabilizes gradients for large key dimensions.","gold_chunks":["Given queries Q, keys K and values V, we compute the attention function on a set of queries simultaneously: Attention(Q, K, V) = softmax(QKᵀ/√dₖ)V.","The scaling factor √dₖ prevents the dot products from growing large in magnitude."]},
{"paper":"Attention Is All You Need","question":"Why use multi-head attention?","answer":"Multiple heads let the model attend to information from different representation subspaces at different positions.","gold_chunks":["Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.","The heads clearly learned to perform different tasks."]},
{"paper":"Attention Is All You Need","question":"How is position encoded in the Transformer?","answer":"By adding sinusoidal positional encodings to token embeddings so the model knows order and relative offsets.","gold_chunks":["To make use of the order of the sequence, we add 'positional encodings' to the input embeddings at the bottoms of the encoder and decoder stacks.","We use sine and cosine functions of different frequencies as positional encodings."]},
{"paper":"Attention Is All You Need","question":"What is the encoder layer structure?","answer":"Each encoder layer has multi-head self-attention followed by a position-wise feed-forward network, with residual connections and layer normalization.","gold_chunks":["The encoder is composed of a stack of N=6 identical layers.","Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."]},
{"paper":"Attention Is All You Need","question":"Why is masking used in the decoder's self-attention?","answer":"To prevent a position from attending to future tokens, enabling autoregressive generation.","gold_chunks":["We mask out (set to −∞) all values in the input of the softmax which correspond to illegal connections (i.e., positions that correspond to subsequent tokens).","This masking, combined with shifting, ensures that the predictions for position i can depend only on the known outputs at positions less than i."]},
{"paper":"Attention Is All You Need","question":"What is the purpose of encoder-decoder attention?","answer":"It lets the decoder attend to the encoder's outputs to condition generation on the source sequence.","gold_chunks":["In addition to the masked self-attention sub-layer, each decoder layer contains a third sub-layer which performs multi-head attention over the output of the encoder stack.","This allows every position in the decoder to attend over all positions in the input sequence."]},
{"paper":"Attention Is All You Need","question":"Describe the position-wise feed-forward network.","answer":"It's the same two-layer MLP applied to each position independently to mix features non-linearly.","gold_chunks":["In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, applied to each position separately and identically.","FFN(x)=max(0, xW1+b1)W2+b2."]},
{"paper":"Attention Is All You Need","question":"Why are residual connections and layer normalization used?","answer":"They stabilize and speed up training by preserving signal flow and keeping activations well-conditioned.","gold_chunks":["We employ a residual connection around each of the two sub-layers, followed by layer normalization.","The output of each sub-layer is LayerNorm(x + Sublayer(x))."]},
{"paper":"Attention Is All You Need","question":"Which datasets demonstrated the Transformer's effectiveness?","answer":"WMT14 English–German and English–French translation benchmarks.","gold_chunks":["We report BLEU scores on WMT 2014 English-to-German and English-to-French translation tasks.","Our 'base' and 'big' models achieve state-of-the-art or competitive results."]},
{"paper":"Attention Is All You Need","question":"How does self-attention enable parallelism?","answer":"All positions are processed simultaneously, avoiding the time-step dependencies of RNNs.","gold_chunks":["Self-attention connects all positions with constant number of sequentially executed operations, whereas recurrent layers require O(n) sequential operations.","This greatly increases training throughput."]},
{"paper":"Attention Is All You Need","question":"What is the time complexity of self-attention over sequence length n?","answer":"O(n²·d) operations per layer due to all-pairs interactions.","gold_chunks":["We compare the complexity per layer, the amount of computation that can be parallelized, and the path length. Self-Attention has O(n²·d) complexity.","Despite the quadratic term, self-attention has constant path length and high parallelism."]},
{"paper":"Attention Is All You Need","question":"What is label smoothing used for in training?","answer":"It regularizes the model by preventing it from becoming over-confident, improving BLEU and generalization.","gold_chunks":["We used label smoothing of value εls=0.1. This improves accuracy and BLEU score.","Label smoothing encourages the model to be less confident."]},
{"paper":"Attention Is All You Need","question":"Which optimizer and schedule are used?","answer":"Adam with β1=0.9, β2=0.98 and a learning-rate schedule with warmup steps ('Noam' schedule).","gold_chunks":["We used the Adam optimizer with β1=0.9, β2=0.98 and ε=10−9.","The learning rate increases linearly for the first warmup steps, and then decreases proportionally to the inverse square root of the step number."]},
{"paper":"Attention Is All You Need","question":"What sizes are presented in the paper?","answer":"A base model and a larger 'big' model with increased dmodel and number of heads.","gold_chunks":["We present base and big versions of the Transformer differing in model dimension and number of heads.","The big model achieves higher BLEU at increased compute."]},
{"paper":"Attention Is All You Need","question":"What is the intuition behind multi-head attention improving modeling power?","answer":"Different heads can specialize to capture diverse relationships such as syntax, position, and long-range dependencies.","gold_chunks":["The heads clearly learned to perform different tasks; attention patterns show syntactic and positional behaviors.","Concatenating multiple heads allows richer representations."]},
{"paper":"Attention Is All You Need","question":"How are embeddings handled between encoder and decoder?","answer":"Source and target embeddings are learned and shared with the pre-softmax linear transformation in some settings.","gold_chunks":["We use learned embeddings to convert input tokens and output tokens to vectors of dimension dmodel.","In our base models, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation."]},
{"paper":"Attention Is All You Need","question":"Why sinusoidal encodings instead of learned ones?","answer":"They let the model extrapolate to longer sequences and infer relative positions easily.","gold_chunks":["We chose sinusoidal functions so that for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.","This allows the model to easily learn to attend by relative positions."]},
{"paper":"Attention Is All You Need","question":"Which regularization techniques besides label smoothing are used?","answer":"Dropout is applied to attention weights, residual connections, and after embeddings.","gold_chunks":["We apply dropout to the output of each sub-layer, to the sums of the embeddings and the positional encodings, and to attention weights.","Dropout rate was 0.1 in base models."]},
{"paper":"Attention Is All You Need","question":"What practical benefit did the authors highlight over recurrent models?","answer":"Significantly faster training from parallelism while achieving SOTA translation quality.","gold_chunks":["By avoiding recurrence we achieve significantly more parallelization and thus shorter training times.","We obtain BLEU improvements on WMT14 and competitive results on other tasks."]},
{"paper":"Attention Is All You Need","question":"What is the attention mechanism?","answer":"A mechanism that allows the model to focus on different parts of the input when producing each part of the output by computing weighted combinations based on learned compatibility scores.","gold_chunks":["An attention function maps a query and set of key-value pairs to an output, where the output is computed as a weighted sum of the values.","The weight assigned to each value is computed by a compatibility function of the query with the corresponding key."]},
{"paper":"Attention Is All You Need","question":"What are queries, keys, and values in attention?","answer":"Queries determine what to look for, keys determine what is available, and values contain the actual information to aggregate.","gold_chunks":["In self-attention, queries, keys and values are all derived from the same input sequence.","The attention weights determine how much each value contributes to the output."]},
{"paper":"Attention Is All You Need","question":"How many layers are in the Transformer encoder and decoder?","answer":"Both encoder and decoder have 6 identical layers stacked.","gold_chunks":["The encoder is composed of a stack of N=6 identical layers.","The decoder is also composed of a stack of N=6 identical layers."]},
{"paper":"Attention Is All You Need","question":"What is the model dimension in the base Transformer?","answer":"512 (dmodel=512)","gold_chunks":["All sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel=512.","The base model uses dmodel=512 and the big model uses dmodel=1024."]},
{"paper":"Attention Is All You Need","question":"How many attention heads does the base model use?","answer":"8 parallel attention heads","gold_chunks":["We use h=8 parallel attention layers, or heads, in our experiments.","For each of these we use dk=dv=dmodel/h=64."]},
{"paper":"Attention Is All You Need","question":"What is the dimension of each attention head?","answer":"64 (dk=dv=64 for 8 heads with dmodel=512)","gold_chunks":["For each of these we use dk=dv=dmodel/h=64.","Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."]},
{"paper":"Attention Is All You Need","question":"What activation function is used in the feed-forward network?","answer":"ReLU (max(0,x))","gold_chunks":["FFN(x)=max(0, xW1+b1)W2+b2.","The feed-forward network applies a ReLU activation between two linear transformations."]},
{"paper":"Attention Is All You Need","question":"What is the hidden dimension of the feed-forward network?","answer":"2048 in the base model","gold_chunks":["While the dimensionality of the model is dmodel=512, the inner-layer has dimensionality dff=2048.","The feed-forward network expands to 2048 dimensions before projecting back to 512."]},
{"paper":"Attention Is All You Need","question":"How is the output generated in the decoder?","answer":"The decoder output is passed through a linear layer and softmax to produce next-token probabilities.","gold_chunks":["The decoder output is passed through a linear transformation and softmax to predict the next token.","We share the weight matrix between the embedding layers and the pre-softmax linear transformation."]},
{"paper":"Attention Is All You Need","question":"What is autoregressive generation?","answer":"Generating one token at a time, where each prediction depends only on previously generated tokens.","gold_chunks":["The Transformer decoder is autoregressive, consuming the previously generated symbols when generating the next.","Masking ensures predictions for position i depend only on known outputs at positions less than i."]},
{"paper":"Attention Is All You Need","question":"Why is the scaling factor sqrt(dk) necessary?","answer":"Without scaling, large dot products push softmax into regions with tiny gradients, hurting training.","gold_chunks":["The scaling factor √dₖ prevents the dot products from growing large in magnitude.","For large values of dk, the dot products grow large, pushing the softmax into regions with extremely small gradients."]},
{"paper":"Attention Is All You Need","question":"What is self-attention?","answer":"Attention where queries, keys, and values all come from the same sequence, allowing each position to attend to all positions in the same sequence.","gold_chunks":["Self-attention allows modeling dependencies between all positions in the input sequence in a single layer.","In self-attention all of the keys, values and queries come from the same place."]},
{"paper":"Attention Is All You Need","question":"How does the Transformer handle variable-length sequences?","answer":"Through attention masks and positional encodings, processing all positions in parallel regardless of sequence length.","gold_chunks":["Self-attention connects all positions with a constant number of operations.","The model can handle sequences of any length up to the maximum position encoding."]},
{"paper":"Attention Is All You Need","question":"What is the maximum sequence length in the Transformer?","answer":"Typically 512 tokens, determined by the positional encoding range.","gold_chunks":["We trained on sequences of length up to 512 tokens.","The positional encodings can extrapolate to longer sequences than seen during training."]},
{"paper":"Attention Is All You Need","question":"What is the difference between encoder and decoder self-attention?","answer":"Encoder uses bidirectional attention (attends to all positions), decoder uses masked attention (only attends to previous positions).","gold_chunks":["The encoder contains self-attention layers where all positions can attend to all other positions.","In the decoder, the self-attention is masked to prevent positions from attending to subsequent positions."]},
{"paper":"Attention Is All You Need","question":"How many parameters does the base Transformer have?","answer":"Approximately 65 million parameters","gold_chunks":["The base model has approximately 65M parameters.","The big model has approximately 213M parameters."]},
{"paper":"Attention Is All You Need","question":"What is the training time for the base model?","answer":"12 hours on 8 P100 GPUs","gold_chunks":["The base models were trained for 100,000 steps or 12 hours on 8 P100 GPUs.","The big models were trained for 300,000 steps (3.5 days) on 8 P100 GPUs."]},
{"paper":"Attention Is All You Need","question":"What BLEU score did the big model achieve on WMT14 English-German?","answer":"28.4 BLEU, a new state-of-the-art at the time","gold_chunks":["Our big model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task.","This improves over the previous best results by over 2.0 BLEU."]},

{"paper":"BERT","question":"What is the core pretraining objective of BERT?","answer":"Masked Language Modeling predicts randomly masked tokens using bidirectional context.","gold_chunks":["We mask 15% of WordPiece tokens and train the model to predict the original token.","BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context."]},
{"paper":"BERT","question":"What additional pretraining task did the original paper include?","answer":"Next Sentence Prediction to model inter-sentence coherence.","gold_chunks":["We jointly pre-train on two tasks: (i) masked LM and (ii) next sentence prediction (NSP).","For NSP we construct sentence pairs and decide whether the second sentence is the actual next sentence."]},
{"paper":"BERT","question":"Which corpora were used to pretrain BERT?","answer":"BooksCorpus and English Wikipedia.","gold_chunks":["We pre-train on the concatenation of BooksCorpus (800M words) and Wikipedia (2,500M words).","We remove lists, tables, and headers from Wikipedia."]},
{"paper":"BERT","question":"What model sizes did BERT introduce?","answer":"BERT_BASE (L=12, H=768, A=12) and BERT_LARGE (L=24, H=1024, A=16).","gold_chunks":["We consider two model sizes: BASE with 12-layer, 768-hidden, 12-heads and LARGE with 24-layer, 1024-hidden, 16-heads.","BERT_BASE has ~110M parameters; BERT_LARGE ~340M."]},
{"paper":"BERT","question":"How are [CLS] and [SEP] used?","answer":"[CLS] provides a pooled sequence representation; [SEP] separates segments and marks boundaries.","gold_chunks":["We use the first token of every sequence as the aggregate representation ([CLS]).","[SEP] is used to separate sentence A and sentence B or mark the end of a single sequence."]},
{"paper":"BERT","question":"Describe the masking policy for MLM.","answer":"Of the 15% selected tokens: 80% become [MASK], 10% become random tokens, and 10% are left unchanged.","gold_chunks":["When masking we replace the word with [MASK] 80% of the time, a random word 10%, and keep it unchanged 10%.","This avoids a mismatch between pre-training and fine-tuning."]},
{"paper":"BERT","question":"What tokenizer does BERT use and why?","answer":"WordPiece with a ~30k vocab to handle open vocabulary via subwords.","gold_chunks":["We use a WordPiece tokenization with a 30,000 token vocabulary.","Subword modeling allows representing rare words as compositions of subwords."]},
{"paper":"BERT","question":"What is BERT's input representation?","answer":"The sum of token, segment, and positional embeddings.","gold_chunks":["The input representation is constructed by summing the corresponding token, segment, and position embeddings.","Segment embeddings distinguish sentence A and sentence B."]},
{"paper":"BERT","question":"How is BERT adapted to downstream tasks?","answer":"By adding small task-specific layers and fine-tuning the whole model end-to-end.","gold_chunks":["For each task we add minimal output layers and fine-tune all parameters jointly.","BERT obtains strong results on a broad range of tasks by fine-tuning."]},
{"paper":"BERT","question":"Which benchmarks showcased BERT's improvements?","answer":"GLUE, SQuAD, and MultiNLI where it set new state of the art at release.","gold_chunks":["BERT significantly outperforms previous models on GLUE and sets new state-of-the-art results on SQuAD v1.1 and v2.0.","Large gains over ELMo and GPT are reported."]},
{"paper":"BERT","question":"Why is bidirectional context important in BERT?","answer":"It lets each token attend to both left and right context, capturing richer dependencies than unidirectional models.","gold_chunks":["Unlike left-to-right models, BERT conditions on both left and right context for each masked token.","This yields a deep bidirectional representation."]},
{"paper":"BERT","question":"Is BERT encoder-only, decoder-only, or encoder-decoder?","answer":"It is an encoder-only Transformer.","gold_chunks":["BERT uses the Transformer encoder architecture with bidirectional self-attention.","No autoregressive decoder is used for pre-training."]},
{"paper":"BERT","question":"How is [CLS] used for classification tasks?","answer":"The pooled [CLS] representation feeds a softmax classification layer.","gold_chunks":["For sentence-level tasks we use the final hidden state of the [CLS] token as the aggregate representation.","A simple classification layer on top of [CLS] is fine-tuned with the model."]},
{"paper":"BERT","question":"What learning rates and optimization strategy are typical for BERT fine-tuning?","answer":"Adam with warmup and linear decay; small learning rates (e.g., 2e-5–5e-5) work well.","gold_chunks":["We fine-tune with Adam and a learning rate warmup over the first 10% of steps then linear decay.","Small learning rates are generally effective across tasks."]},
{"paper":"BERT","question":"How does BERT handle sentence pairs?","answer":"By using segment embeddings to distinguish sentence A and B and training with NSP.","gold_chunks":["We pack sentence pairs into a single sequence separated by [SEP] and add segment embeddings.","NSP encourages the model to capture inter-sentence coherence."]},
{"paper":"BERT","question":"What regularization techniques are used during pretraining?","answer":"Dropout and small weight decay; dropout is applied to all layers.","gold_chunks":["We apply dropout to all layers in the Transformer encoder.","Weight decay of 0.01 is used during training."]},
{"paper":"BERT","question":"How big is the pretraining batch/sequence setup?","answer":"Sequences up to 512 tokens with large batches; curriculum from shorter to longer sequences is used.","gold_chunks":["We pre-train on sequences of 128 and 512 tokens, first training on shorter sequences then continuing on longer ones.","Large batch training is employed for efficiency."]},
{"paper":"BERT","question":"What is the principal advantage of fine-tuning over feature-based transfer?","answer":"End-to-end fine-tuning adapts the entire encoder to the task, yielding larger gains than using frozen features.","gold_chunks":["We compare fine-tuning to feature-based approaches and find fine-tuning generally performs better.","Task-specific heads are shallow; most capacity lives in the pretrained encoder."]},
{"paper":"BERT","question":"What does BERT stand for?","answer":"Bidirectional Encoder Representations from Transformers","gold_chunks":["BERT stands for Bidirectional Encoder Representations from Transformers.","The key innovation is pre-training deep bidirectional representations."]},
{"paper":"BERT","question":"How does BERT differ from GPT?","answer":"BERT is bidirectional using masked language modeling, while GPT is unidirectional using left-to-right language modeling.","gold_chunks":["Unlike GPT which uses left-to-right language modeling, BERT uses masked language modeling to enable bidirectional pre-training.","This bidirectional pre-training is the key difference from previous approaches."]},
{"paper":"BERT","question":"What is masked language modeling?","answer":"Randomly masking tokens in the input and training the model to predict the original tokens based on context.","gold_chunks":["We mask 15% of WordPiece tokens and train the model to predict the original token.","This allows the model to fuse both left and right context."]},
{"paper":"BERT","question":"What percentage of tokens are masked during MLM training?","answer":"15% of all WordPiece tokens","gold_chunks":["We mask 15% of WordPiece tokens at random.","This masking ratio balances sufficient training signal with computational efficiency."]},
{"paper":"BERT","question":"Why does BERT use 80-10-10 masking strategy?","answer":"To reduce mismatch between pretraining (with [MASK]) and fine-tuning (without [MASK]).","gold_chunks":["When masking we replace the word with [MASK] 80% of the time, a random word 10%, and keep it unchanged 10%.","This avoids a mismatch between pre-training and fine-tuning."]},
{"paper":"BERT","question":"What is Next Sentence Prediction?","answer":"A binary classification task predicting whether sentence B actually follows sentence A in the original text.","gold_chunks":["For NSP we construct sentence pairs and decide whether the second sentence is the actual next sentence.","50% of the time B is the actual next sentence, 50% it is a random sentence."]},
{"paper":"BERT","question":"What is the purpose of segment embeddings?","answer":"To distinguish between sentence A and sentence B in sentence-pair tasks.","gold_chunks":["Segment embeddings distinguish sentence A and sentence B.","We add a learned segment embedding to every token indicating whether it belongs to sentence A or B."]},
{"paper":"BERT","question":"How many parameters does BERT_BASE have?","answer":"Approximately 110 million parameters","gold_chunks":["BERT_BASE has ~110M parameters.","The base model has 12 layers, 768 hidden dimensions, and 12 attention heads."]},
{"paper":"BERT","question":"How many parameters does BERT_LARGE have?","answer":"Approximately 340 million parameters","gold_chunks":["BERT_LARGE ~340M.","The large model has 24 layers, 1024 hidden dimensions, and 16 attention heads."]},
{"paper":"BERT","question":"What is the vocabulary size of BERT?","answer":"Approximately 30,000 WordPiece tokens","gold_chunks":["We use a WordPiece tokenization with a 30,000 token vocabulary.","The vocabulary is built using WordPiece tokenization."]},
{"paper":"BERT","question":"What is the maximum sequence length for BERT?","answer":"512 tokens","gold_chunks":["We pre-train on sequences of 128 and 512 tokens.","The maximum sequence length is 512 tokens."]},
{"paper":"BERT","question":"How long does BERT pretraining take?","answer":"BERT_BASE: 4 days on 4 Cloud TPUs; BERT_LARGE: 4 days on 16 Cloud TPUs","gold_chunks":["Pre-training is computationally expensive but only needs to be done once.","The base model was pre-trained on 4 Cloud TPUs for 4 days."]},
{"paper":"BERT","question":"What is the pretraining corpus size?","answer":"About 3.3 billion words (800M from BooksCorpus + 2.5B from Wikipedia)","gold_chunks":["We pre-train on the concatenation of BooksCorpus (800M words) and Wikipedia (2,500M words).","The total corpus contains about 3.3 billion words."]},
{"paper":"BERT","question":"How does BERT handle question answering?","answer":"By predicting start and end positions of the answer span in the context.","gold_chunks":["For question answering we learn start and end vectors that mark the answer span.","The [CLS] representation is not used for QA; instead we predict span boundaries."]},
{"paper":"BERT","question":"What is the dropout rate used in BERT?","answer":"0.1 (10%)","gold_chunks":["We apply dropout to all layers in the Transformer encoder.","A dropout probability of 0.1 is used on all layers."]},
{"paper":"BERT","question":"What optimizer does BERT use?","answer":"Adam optimizer","gold_chunks":["We fine-tune with Adam.","The Adam optimizer is used with specific learning rate schedules."]},
{"paper":"BERT","question":"What is the warm-up ratio for BERT fine-tuning?","answer":"10% of total training steps","gold_chunks":["We fine-tune with Adam and a learning rate warmup over the first 10% of steps.","After warmup, the learning rate decays linearly."]},
{"paper":"BERT","question":"What batch size is used for BERT pretraining?","answer":"256 sequences","gold_chunks":["Large batch training is employed for efficiency.","We use a batch size of 256 sequences."]},
{"paper":"BERT","question":"How many training steps for BERT pretraining?","answer":"1 million steps","gold_chunks":["We train for 1,000,000 steps.","Pre-training takes approximately 1 million optimization steps."]},
{"paper":"BERT","question":"What improvements did BERT show on SQuAD?","answer":"BERT_LARGE achieved 93.2 F1 on SQuAD 1.1, a 1.5 point improvement over previous best.","gold_chunks":["BERT sets new state-of-the-art results on SQuAD v1.1 and v2.0.","BERT obtains an F1 score of 93.2, establishing a new state of the art."]},
{"paper":"BERT","question":"What is the GLUE benchmark?","answer":"A collection of 9 natural language understanding tasks for evaluating language models.","gold_chunks":["GLUE is a collection of diverse natural language understanding tasks.","BERT significantly outperforms previous models on GLUE."]},
{"paper":"BERT","question":"Can BERT be used for feature extraction?","answer":"Yes, by using the fixed BERT representations as features for a downstream task-specific model.","gold_chunks":["We compare fine-tuning to feature-based approaches.","BERT can be used as a feature extractor by freezing the weights."]},
{"paper":"BERT","question":"What is the difference between BERT and ELMo?","answer":"BERT uses bidirectional Transformers while ELMo uses bidirectional LSTMs; BERT's representations are deeper and more contextual.","gold_chunks":["Large gains over ELMo are reported.","Unlike ELMo's shallow concatenation of left-to-right and right-to-left LSTMs, BERT deeply fuses bidirectional context."]},

{"paper":"CLIP","question":"How does CLIP support image-text retrieval?","answer":"Both encoders map to the same space, so nearest-neighbor search retrieves the matching caption for an image and vice versa.","gold_chunks":["By aligning modalities, we can perform text→image and image→text retrieval using cosine similarity.","This unified space enables cross-modal tasks beyond classification."]},
{"paper":"CLIP","question":"What preprocessing is applied to text and images?","answer":"Images are resized/cropped and texts are tokenized (e.g., BPE) before encoding.","gold_chunks":["We apply standard image augmentations and tokenize texts with a Transformer tokenizer.","Preprocessing normalizes inputs for large-scale training."]},
{"paper":"CLIP","question":"Why does CLIP scale well?","answer":"Simple contrastive loss, efficient encoders, and web-scale data allow training to benefit from more compute and data.","gold_chunks":["We find performance improves smoothly with model size, data, and compute.","The training objective and architecture are straightforward to scale."]},
{"paper":"CLIP","question":"What does zero-shot mean in CLIP?","answer":"Evaluating on new label sets without task-specific fine-tuning by using text prompts as classifiers.","gold_chunks":["Zero-shot classification replaces a learned classifier with text prompts for labels.","No gradient updates on the downstream dataset are performed."]},
{"paper":"CLIP","question":"How does prompt ensembling help?","answer":"Averaging text embeddings from multiple templates reduces variance and lifts accuracy.","gold_chunks":["We evaluate multiple templates per class and average the resulting text embeddings.","This mitigates sensitivity to prompt wording."]},
{"paper":"CLIP","question":"How can CLIP be adapted beyond zero-shot?","answer":"By linear probing or fine-tuning heads while keeping encoders frozen, depending on the task.","gold_chunks":["Besides zero-shot, we report linear probe results as a strong baseline.","Selective fine-tuning can further improve specific tasks."]},
{"paper":"CLIP","question":"What alignment is CLIP learning between modalities?","answer":"A shared embedding geometry where semantically matching images and texts are nearby and non-matches are far apart.","gold_chunks":["Joint contrastive training brings paired representations together and pushes non-paired ones apart.","This alignment enables retrieval, zero-shot classification, and other cross-modal tasks."]},
{"paper":"CLIP","question":"What factors most influence CLIP's zero-shot accuracy?","answer":"Prompt design, model capacity, data scale, and the quality/diversity of pretraining pairs.","gold_chunks":["We observe sensitivity to prompt phrasing and improvements from ensembling templates.","Larger ViT backbones and more diverse data boost performance."]},
{"paper":"CLIP","question":"Why is CLIP often robust to spurious dataset biases?","answer":"Natural-language supervision exposes the model to a broader set of concepts than fixed label sets, improving generalization.","gold_chunks":["Compared to supervised ImageNet classification, CLIP exhibits stronger robustness on several out-of-distribution evaluations.","Exposure to varied captions reduces overfitting to narrow label taxonomies."]},
{"paper":"CLIP","question":"What does CLIP stand for?","answer":"Contrastive Language-Image Pre-training","gold_chunks":["CLIP stands for Contrastive Language-Image Pre-training.","The method uses contrastive learning to align vision and language."]},
{"paper":"CLIP","question":"What is contrastive learning in CLIP?","answer":"A learning approach where matching image-text pairs are pulled together in embedding space while non-matching pairs are pushed apart.","gold_chunks":["We train an image encoder and a text encoder jointly to predict which caption goes with which image.","The contrastive objective benefits from many negatives."]},
{"paper":"CLIP","question":"What is the WIT dataset?","answer":"WebImageText, a dataset of 400 million image-text pairs collected from the internet for CLIP pretraining.","gold_chunks":["We construct a dataset of about 400 million (image, text) pairs collected from publicly available sources on the Internet.","WIT is sourced from diverse web pages to ensure broad coverage."]},
{"paper":"CLIP","question":"How does CLIP differ from supervised ImageNet models?","answer":"CLIP uses natural language supervision from image-text pairs instead of fixed classification labels, enabling zero-shot transfer.","gold_chunks":["Natural-language supervision provides broader coverage than fixed label sets.","CLIP can generalize to new visual concepts described in text without retraining."]},
{"paper":"CLIP","question":"What Vision Transformer variants does CLIP use?","answer":"ViT-B/32, ViT-B/16, ViT-L/14, and ViT-L/14@336px","gold_chunks":["We evaluate Vision Transformers (e.g., ViT-B/32, ViT-L/14) as image encoders.","Larger ViT models generally improve performance."]},
{"paper":"CLIP","question":"What is the text encoder architecture in CLIP?","answer":"A Transformer with 12 layers, 512 width, and 8 attention heads, operating on BPE tokens.","gold_chunks":["The text encoder is a Transformer operating on byte-pair encoded tokens.","The text encoder uses a maximum sequence length of 76 tokens."]},
{"paper":"CLIP","question":"What is the maximum text length for CLIP?","answer":"76 BPE tokens","gold_chunks":["The text encoder uses a maximum sequence length of 76 tokens.","Longer texts are truncated to fit this limit."]},
{"paper":"CLIP","question":"What tokenization does CLIP use for text?","answer":"Byte Pair Encoding (BPE) with a vocabulary of 49,152 tokens","gold_chunks":["The text encoder is a Transformer operating on byte-pair encoded tokens.","We use a case-insensitive BPE tokenizer with a vocabulary size of 49,152."]},
{"paper":"CLIP","question":"What batch size does CLIP use during training?","answer":"32,768 image-text pairs per batch","gold_chunks":["We scale batch size to increase the number of negative examples per step.","Large batch training is important for performance with batch sizes up to 32,768."]},
{"paper":"CLIP","question":"How many epochs does CLIP train for?","answer":"32 epochs over the 400M image-text pairs","gold_chunks":["We train for 32 epochs.","Training takes approximately 12 days on 256 GPUs."]},
{"paper":"CLIP","question":"What optimizer does CLIP use?","answer":"Adam optimizer with decoupled weight decay (AdamW)","gold_chunks":["We use the Adam optimizer with decoupled weight decay regularization.","Learning rate is scheduled with cosine decay."]},
{"paper":"CLIP","question":"What image resolution does CLIP use?","answer":"224×224 pixels for most models, with 336×336 for ViT-L/14@336px","gold_chunks":["Images are resized and center-cropped to 224×224.","For ViT-L/14@336px we use 336×336 resolution."]},
{"paper":"CLIP","question":"What data augmentation does CLIP use?","answer":"Only random square cropping, no other augmentation to preserve text-image correspondence","gold_chunks":["We apply standard image augmentations.","Unlike supervised learning, we use minimal augmentation to preserve the natural image-text pairing."]},
{"paper":"CLIP","question":"How does CLIP handle multi-modal understanding?","answer":"By learning a joint embedding space where images and their textual descriptions are close together.","gold_chunks":["This learns a shared embedding space where matching pairs are close and mismatches are far.","The unified space enables cross-modal tasks."]},
{"paper":"CLIP","question":"What is linear probing in CLIP?","answer":"Training only a linear classifier on top of frozen CLIP image features for a specific task.","gold_chunks":["We train only a linear classifier on top of frozen image features and achieve competitive performance.","This isolates representation quality from prompt design."]},
{"paper":"CLIP","question":"What datasets is CLIP evaluated on?","answer":"ImageNet and 26 other datasets covering various visual tasks like object recognition, OCR, action recognition, and geo-localization.","gold_chunks":["We evaluate on 27 different datasets spanning various tasks.","CLIP shows strong zero-shot performance across diverse benchmarks."]},
{"paper":"CLIP","question":"How does CLIP compare to supervised ResNet-50 on ImageNet?","answer":"CLIP ResNet-50 matches the original ResNet-50's accuracy in zero-shot mode without using any ImageNet training data.","gold_chunks":["Our ResNet-50 CLIP model matches the accuracy of the original supervised ResNet-50.","This is achieved zero-shot without seeing any ImageNet examples."]},
{"paper":"CLIP","question":"What is prompt engineering in CLIP?","answer":"Carefully designing text templates to convert class names into natural language descriptions for better zero-shot performance.","gold_chunks":["Different phrasings for the same label can produce noticeably different performance.","We use templated prompts like 'a photo of a [class]' instead of just class names."]},
{"paper":"CLIP","question":"Why use 'a photo of a [class]' instead of just '[class]'?","answer":"The natural language context helps CLIP's text encoder produce better embeddings that align with actual image captions.","gold_chunks":["We use templated prompts and average over multiple templates to stabilize zero-shot accuracy.","Prompts like 'a photo of a dog' perform better than just 'dog' because they match the training data distribution."]},
{"paper":"CLIP","question":"What is the InfoNCE loss?","answer":"A contrastive loss that maximizes agreement between positive pairs while minimizing agreement with negative pairs using a softmax over similarities.","gold_chunks":["Each image is matched to its caption and vice versa via a softmax over pairwise similarities within the batch.","The InfoNCE objective encourages the model to identify correct image-text pairs."]},
{"paper":"CLIP","question":"How does CLIP handle class imbalance?","answer":"Through natural language supervision which provides more balanced and diverse coverage than fixed label distributions.","gold_chunks":["Natural-language supervision provides broader coverage than fixed label sets.","Web data naturally covers long-tail concepts better than curated datasets."]},
{"paper":"CLIP","question":"What computational cost does CLIP have?","answer":"Training the largest model (ViT-L/14) requires approximately 592 GPU days (256 V100 GPUs for 12 days).","gold_chunks":["Training takes approximately 12 days on 256 GPUs.","The computational requirements scale with model size and dataset size."]},
{"paper":"CLIP","question":"How does CLIP perform on fine-grained classification?","answer":"Generally weaker than on broad categories, as web captions rarely describe fine-grained distinctions.","gold_chunks":["CLIP struggles more on fine-grained tasks like distinguishing car models or flower species.","Web captions tend to describe high-level concepts rather than subtle visual differences."]},
{"paper":"CLIP","question":"What is CLIP's performance on ImageNet zero-shot?","answer":"76.2% top-1 accuracy with ViT-L/14@336px, competitive with supervised models.","gold_chunks":["Our best model achieves 76.2% zero-shot accuracy on ImageNet.","This rivals the accuracy of the original ResNet-50 trained on ImageNet."]},
{"paper":"CLIP","question":"How does CLIP handle abstract or complex queries?","answer":"By leveraging the compositional nature of language to understand complex descriptions through its text encoder.","gold_chunks":["The text encoder can understand complex compositional descriptions.","Language provides a flexible interface for specifying visual concepts."]},
{"paper":"CLIP","question":"What are CLIP's main limitations?","answer":"Struggles with fine-grained classification, counting objects, and tasks requiring spatial reasoning or systematic generalization.","gold_chunks":["CLIP struggles more on fine-grained tasks.","Performance on counting and spatial reasoning is limited."]},
{"paper":"CLIP","question":"How does CLIP handle out-of-distribution data?","answer":"Shows improved robustness compared to supervised models due to exposure to diverse web data during pretraining.","gold_chunks":["We evaluate on several distribution shift datasets and find CLIP zero-shot models are competitive or superior to supervised baselines.","Exposure to varied captions reduces overfitting to narrow label taxonomies."]},
{"paper":"CLIP","question":"What is the symmetric loss in CLIP?","answer":"Both image-to-text and text-to-image directions are optimized simultaneously in the contrastive loss.","gold_chunks":["Each image is matched to its caption and vice versa via a softmax over pairwise similarities within the batch.","The symmetric formulation improves both retrieval directions."]},
{"paper":"CLIP","question":"How does CLIP relate to few-shot learning?","answer":"CLIP can be adapted to few-shot learning by using the few examples to construct better prompts or by fine-tuning on them.","gold_chunks":["CLIP can be adapted with few-shot learning techniques.","Even a few examples can significantly improve performance on specific tasks."]},
{"paper":"CLIP","question":"What ethical considerations does CLIP raise?","answer":"Potential for bias amplification from web data, surveillance applications, and generation of problematic content.","gold_chunks":["CLIP inherits biases present in its training data from the internet.","Care must be taken when deploying CLIP in sensitive applications."]},
{"paper":"CLIP","question":"How does CLIP handle multilingual text?","answer":"The base model is trained primarily on English, but multilingual variants can be trained on diverse language data.","gold_chunks":["The text encoder is trained primarily on English captions.","Multilingual extensions are possible by training on captions in multiple languages."]},
{"paper":"CLIP","question":"What is the relationship between CLIP and vision-language models?","answer":"CLIP is a foundational vision-language model that aligns visual and textual representations for downstream tasks.","gold_chunks":["CLIP learns a joint embedding space for vision and language.","This alignment enables various vision-language tasks like retrieval and zero-shot classification."]},
{"paper":"CLIP","question":"How does CLIP compare to ALIGN?","answer":"ALIGN uses a similar contrastive approach but with 1.8B noisier image-text pairs, achieving comparable or better results.","gold_chunks":["Contemporary work like ALIGN demonstrates similar ideas at larger scale.","Both use contrastive learning but differ in data curation strategies."]},
{"paper":"CLIP","question":"What is the temperature parameter value in CLIP?","answer":"A learnable parameter initialized to 0.07 (corresponding to τ=1/0.07≈14.3)","gold_chunks":["A learned temperature parameter scales the logits in the contrastive loss.","We optimize τ jointly with encoder parameters, initialized to 0.07."]},
{"paper":"CLIP","question":"How does CLIP handle ambiguous images?","answer":"By comparing against multiple text descriptions and selecting the highest similarity, leveraging the flexibility of natural language.","gold_chunks":["CLIP can handle ambiguous cases by comparing against diverse text descriptions.","The text encoder's flexibility allows nuanced disambiguation."]},
{"paper":"CLIP","question":"What preprocessing is done to the CLIP dataset?","answer":"Minimal filtering to remove very short captions and duplicates, preserving the natural distribution of web data.","gold_chunks":["We apply minimal filtering to the web data to preserve its natural diversity.","Very short captions and exact duplicates are removed."]},
{"paper":"CLIP","question":"How does CLIP's training objective differ from traditional classification?","answer":"CLIP uses contrastive learning over image-text pairs instead of predicting fixed class labels.","gold_chunks":["The contrastive objective benefits from many negatives.","This differs from standard cross-entropy classification over fixed labels."]},
{"paper":"CLIP","question":"What is CLIP's impact on computer vision?","answer":"Enabled zero-shot transfer to new tasks, inspired vision-language models, and showed the power of natural language supervision.","gold_chunks":["CLIP demonstrates that natural language can be an effective supervision signal.","It has inspired numerous follow-up works in vision-language learning."]},
{"paper":"CLIP","question":"How does CLIP handle black and white images?","answer":"The image encoder processes them like any other image, relying on texture and shape rather than color information.","gold_chunks":["CLIP can work with grayscale images as the encoder focuses on visual features beyond color.","Performance may vary depending on how informative color is for the task."]},
{"paper":"CLIP","question":"What is the context length for CLIP's text encoder?","answer":"76 tokens maximum","gold_chunks":["The text encoder uses a maximum sequence length of 76 tokens.","Texts longer than this are truncated."]},
{"paper":"CLIP","question":"How does CLIP perform on OCR tasks?","answer":"Shows reasonable performance on natural OCR tasks due to text in training images, but not specifically optimized for it.","gold_chunks":["CLIP shows some OCR capability from incidental text in training images.","However, it is not specialized for OCR and may underperform dedicated models."]},
{"paper":"CLIP","question":"What makes CLIP's approach scalable?","answer":"Simple architecture, straightforward contrastive loss, and ability to leverage abundant web image-text pairs without manual annotation.","gold_chunks":["We find performance improves smoothly with model size, data, and compute.","The approach scales naturally with available web data."]},
{"paper":"CLIP","question":"How does CLIP handle domain-specific images?","answer":"Performance varies by domain; better on common web domains but may struggle on specialized domains like medical imaging.","gold_chunks":["CLIP's performance depends on domain coverage in the training data.","Specialized domains underrepresented on the web may see lower performance."]},
{"paper":"CLIP","question":"What is CLIP's architecture lineage?","answer":"Combines ResNet/ViT for vision with Transformer for text, building on established architectures.","gold_chunks":["We evaluate CNNs (e.g., ResNet-50) and Vision Transformers (e.g., ViT-B/32, ViT-L/14) as image encoders.","The text encoder is a Transformer."]},
{"paper":"CLIP","question":"How does CLIP enable compositionality?","answer":"Through natural language understanding, allowing combination of multiple concepts in text prompts.","gold_chunks":["The text encoder can understand complex compositional descriptions.","Language naturally enables compositional reasoning."]},
{"paper":"CLIP","question":"What is the embedding dimension in CLIP?","answer":"512 dimensions for the shared image-text embedding space","gold_chunks":["Both encoders project to a shared 512-dimensional embedding space.","Embeddings are L2-normalized before computing similarity."]}
]
